{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swillenson/Developing-and-Designing-Interactive-Devices/blob/master/2-conv1d_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eGZIh3o1xch"
      },
      "source": [
        "# 1D Convolution on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOEQo1pd1ddO"
      },
      "source": [
        "## 1. Set-up "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9xe2GRFj1ddO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c3265a0-93e1-4237-db7a-32cf1e29c70c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive \n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CA1bjoZe1ddP"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username    \n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d_GTOcdm1ddQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c24259d-f609-4163-cc2c-62315b2c9505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/ece5545’: File exists\n",
            "/content/gdrive/MyDrive/ece5545\n",
            "fatal: destination path 'a3-swillenson' already exists and is not an empty directory.\n",
            "/content/gdrive/MyDrive/ece5545/a3-swillenson\n",
            "M\tsrc/ops.py\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "Already up to date.\n",
            "/content/gdrive/MyDrive/ece5545\n"
          ]
        }
      ],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a3-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a3-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a3-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AOFOUrP81ddQ"
      },
      "outputs": [],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0VDBYN2Z1ddS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92086e5b-541e-40dd-c17a-7cb2afa5ce08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-conv1d_cpu.ipynb   4-gemm_gpu.ipynb\t    README.md\n",
            "2-conv1d_gpu.ipynb   5-conv2d_dw_gpu.ipynb  src\n",
            "3-conv1d_fpga.ipynb  leaderboard_id.txt     tests\n"
          ]
        }
      ],
      "source": [
        "!ls {PROJECT_ROOT}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ircHNs-j2DYX"
      },
      "source": [
        "## 2. Install TVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WJpFlXLd1_K5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365d71e3-aba9-4268-e04e-900cfb2f196d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://tlcpack.ai/wheels\n",
            "Collecting tlcpack-nightly-cu102\n",
            "  Downloading https://github.com/tlc-pack/tlcpack/releases/download/v0.12.dev/tlcpack_nightly_cu102-0.12.dev505%2Bga84a2cbe0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (408.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from tlcpack-nightly-cu102) (22.2.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.9/dist-packages (from tlcpack-nightly-cu102) (6.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from tlcpack-nightly-cu102) (5.9.4)\n",
            "Requirement already satisfied: numpy<=1.23 in /usr/local/lib/python3.9/dist-packages (from tlcpack-nightly-cu102) (1.22.4)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from tlcpack-nightly-cu102) (2.2.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from tlcpack-nightly-cu102) (1.10.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tlcpack-nightly-cu102) (4.4.2)\n",
            "Installing collected packages: tlcpack-nightly-cu102\n",
            "Successfully installed tlcpack-nightly-cu102-0.12.dev505+ga84a2cbe0\n"
          ]
        }
      ],
      "source": [
        "!pip install tlcpack-nightly-cu102 -f https://tlcpack.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVzL-yHx1ddS"
      },
      "source": [
        "## 3. Implement `make_conv1d_gpu_scheduler_func` function in `src.ops`\n",
        "\n",
        "In that function, you are required to implemented 1D convolution and use TVM to optimize it.\n",
        "Let $x \\in \\mathbb{R}^m$ and $y \\in \\mathbb{R}^n$, then \n",
        "$$\n",
        "\\operatorname{conv1d}(x, y)_i = \\sum_{j=-\\infty}^{\\infty} x[j]y[i-j], \\forall i \\in \\{0, 1, \\dots, m + n - 1\\}\n",
        "$$\n",
        "\n",
        "Please use zero padding and unit stride. Please see the numpy convolution function for more detail: [link](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html).\n",
        "\n",
        "The `make_conv1d_gpu_scheduler_func` takes $m$ and $n$, which are the size of the two 1D input array. \n",
        "You should return both the TVM scheduler and the TVM opterator for \n",
        "1. Input $x$\n",
        "2. Input $y$\n",
        "3. Output $out$\n",
        "\n",
        "The scheduler should be able to used to build a function with signature $func(x, y, out)$. \n",
        "Please see the following cells for usage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zY2kYOFl1ddT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d28585e-8665-4acb-c573-d88b40ab2d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: [0.05323359 0.28025395 0.9364747  ... 0.8969244  0.6899942  0.5261962 ]\n",
            "Output: [0.05323359 0.28025395 0.9364747  ... 0.8969244  0.6899942  0.5261962 ]\n",
            "1DConv TVM: 4.171775 ms\n"
          ]
        }
      ],
      "source": [
        "import tvm\n",
        "import numpy as np\n",
        "import sys\n",
        "# Adding assignment 3 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "from src.ops import make_conv1d_gpu_scheduler\n",
        "\n",
        "M = 16384\n",
        "N = 32\n",
        "dtype = 'float32'\n",
        "a_np = np.random.rand(M).astype(dtype)\n",
        "w_np = np.random.rand(N).astype(dtype)\n",
        "b_np = np.convolve(a_np, w_np)\n",
        "\n",
        "s, A, W, B = make_conv1d_gpu_scheduler(M, N) \n",
        "func = tvm.build(s, [A, W, B], \"cuda\")\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "a = tvm.nd.array(a_np, dev)\n",
        "w = tvm.nd.array(w_np, dev)\n",
        "b = tvm.nd.array(np.zeros((M+N-1), dtype), dev)\n",
        "func(a, w, b)\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1, repeat =1)\n",
        "\n",
        "\n",
        "print(\"Answer:\", b_np)\n",
        "print(\"Output:\", b)\n",
        "print(f\"1DConv TVM: %f ms\" % (evaluator(a, w, b).mean * 1e3))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "import numpy as np\n",
        "import sys\n",
        "# Adding assignment 3 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "from src.ops import make_conv1d_gpu_scheduler\n",
        "from tvm import te\n",
        "\n",
        "\n",
        "def make_conv1d_gpu_scheduler_optimized1(M, N):\n",
        "    A = te.placeholder((M,), name=\"A\")\n",
        "    W = te.placeholder((N,), name=\"W\")\n",
        "\n",
        "    k = te.reduce_axis((0, M + N - 1), \"k\")\n",
        "    B = te.compute(\n",
        "        (M + N - 1,),\n",
        "        lambda n: te.sum(tvm.tir.if_then_else(\n",
        "            tvm.tir.any(k < 0, k >= M, n - k < 0, n - k >= N),\n",
        "            tvm.tir.const(0.0, \"float32\"),\n",
        "            A[k] * W[n - k]), axis=k),\n",
        "        name=\"B\",\n",
        "    )\n",
        "\n",
        "    s = te.create_schedule(B.op)\n",
        "\n",
        "    # Optimization 1: Loop tiling\n",
        "    block_size = 32\n",
        "    bx, tx = s[B].split(B.op.axis[0], factor=block_size)\n",
        "    s[B].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
        "    s[B].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
        "    return s, A, W, B\n",
        "\n",
        "\n",
        "M = 16384\n",
        "N = 32\n",
        "dtype = 'float32'\n",
        "a_np = np.random.rand(M).astype(dtype)\n",
        "w_np = np.random.rand(N).astype(dtype)\n",
        "b_np = np.convolve(a_np, w_np)\n",
        "\n",
        "s, A, W, B = make_conv1d_gpu_scheduler_optimized1(M, N) \n",
        "\n",
        "func = tvm.build(s, [A, W, B], \"cuda\")\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "a = tvm.nd.array(a_np, dev)\n",
        "w = tvm.nd.array(w_np, dev)\n",
        "b = tvm.nd.array(np.zeros((M+N-1), dtype), dev)\n",
        "func(a, w, b)\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1, repeat =1)\n",
        "\n",
        "\n",
        "print(\"Answer:\", b_np)\n",
        "print(\"Output:\", b)\n",
        "print(f\"1DConv TVM: %f ms\" % (evaluator(a, w, b).mean * 1e3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHRL9j7RUsrG",
        "outputId": "35c82fca-c139-4e41-c674-b7245b767815"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: [0.3211576  0.35567337 0.44408584 ... 1.5107232  0.82552826 0.3141737 ]\n",
            "Output: [0.3211576  0.35567337 0.44408584 ... 1.5107232  0.82552826 0.3141737 ]\n",
            "1DConv TVM: 4.163584 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "import numpy as np\n",
        "import sys\n",
        "# Adding assignment 3 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "from src.ops import make_conv1d_gpu_scheduler\n",
        "from tvm import te\n",
        "\n",
        "\n",
        "def make_conv1d_gpu_scheduler_optimized2(M, N):\n",
        "    A = te.placeholder((4, M), name='A')\n",
        "    W = te.placeholder((N,), name='W')\n",
        "    \n",
        "    # optimize by unrolling the loop over k\n",
        "    k = te.reduce_axis((0, 4), name='k')\n",
        "    B = te.compute(\n",
        "        (M + N - 1,), \n",
        "        lambda n: te.sum(A[k, n - k] * W[k], axis=k), \n",
        "        name='B_unrolled'\n",
        "    )\n",
        "\n",
        "    s = te.create_schedule(B.op)\n",
        "    \n",
        "    # split n by factor 64 for parallelism\n",
        "    n, ni = s[B].split(B.op.axis[0], factor=64)\n",
        "    # split k by factor 4 for vectorization\n",
        "    k, ko = s[B].split(k, factor=4)\n",
        "    # reorder the axes for vectorization\n",
        "    s[B].reorder(ni, ko, k, n)\n",
        "    # vectorize\n",
        "\n",
        "    return s, A, W, B\n",
        "\n",
        "\n",
        "M = 16384\n",
        "N = 32\n",
        "dtype = 'float32'\n",
        "a_np = np.random.rand(M).astype(dtype)\n",
        "w_np = np.random.rand(N).astype(dtype)\n",
        "b_np = np.convolve(a_np, w_np)\n",
        "\n",
        "s, A, W, B = make_conv1d_gpu_scheduler_optimized2(M, N) \n",
        "\n",
        "func = tvm.build(s, [A, W, B], \"cuda\")\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "a = tvm.nd.array(a_np, dev)\n",
        "w = tvm.nd.array(w_np, dev)\n",
        "b = tvm.nd.array(np.zeros((M+N-1), dtype), dev)\n",
        "func(a, w, b)\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1, repeat =1)\n",
        "\n",
        "\n",
        "print(\"Answer:\", b_np)\n",
        "print(\"Output:\", b)\n",
        "print(f\"1DConv TVM: %f ms\" % (evaluator(a, w, b).mean * 1e3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xbJaJGEmU_cE",
        "outputId": "8838b250-2092-46b6-e816-a845e1baca7f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TVMError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-a0329584963c>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_conv1d_gpu_scheduler_optimized2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mdev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tvm/driver/build_module.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(inputs, args, target, target_host, runtime, name, binds)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0mannotated_mods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanon_target_map_and_host\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotated_mods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m     \u001b[0mrt_mod_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_driver_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtir_to_runtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotated_mods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mannotated_mods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanon_target_map_and_host\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotated_mods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_host\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.PackedFuncBase.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./packed_func.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.FuncCall3\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mtvm/_ffi/_cython/./base.pxi\u001b[0m in \u001b[0;36mtvm._ffi._cy3.core.CHECK_CALL\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTVMError\u001b[0m: Traceback (most recent call last):\n  10: TVMFuncCall\n  9: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS0_6ModuleERKNS0_3MapINS_6TargetENS_8IRModuleEvvEES7_EE17AssignTypedLambdaINS_UlSB_S7_E4_EEEvT_SsEUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SH_SL_\n  8: tvm::TIRToRuntime(tvm::runtime::Map<tvm::Target, tvm::IRModule, void, void> const&, tvm::Target const&)\n  7: tvm::SplitMixedModule(tvm::IRModule, tvm::Target const&, tvm::Target const&)\n  6: tvm::ApplyPasses(tvm::IRModule, tvm::transform::Sequential)\n  5: tvm::transform::Pass::operator()(tvm::IRModule) const\n  4: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  3: tvm::transform::SequentialNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  2: tvm::transform::Pass::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  1: tvm::transform::ModulePassNode::operator()(tvm::IRModule, tvm::transform::PassContext const&) const\n  0: _ZN3tvm7runtime13PackedFuncObj9ExtractorINS0_16PackedFuncSubObjIZNS0_15TypedPackedFuncIFNS_8IRModuleES5_NS_9transform11PassContextEEE17AssignTypedLambdaIZNS_3tir9transform12VerifyMemoryEvEUlS5_S7_E_EEvT_EUlRKNS0_7TVMArgsEPNS0_11TVMRetValueEE_EEE4CallEPKS1_SF_SJ_\n  Did you forget to bind?\n    Variable `W` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `A` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `B_unrolled` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `B_unrolled` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n    Variable `B_unrolled` is directly accessed by host memory (it is not contained in a thread environment or in the function arguments.\n  File \"/workspace/tvm/src/tir/analysis/verify_memory.cc\", line 206\nRuntimeError: Memory verification failed with the following errors:\n# from tvm.script import tir as T\n\n@T.prim_func\ndef default_function(A: T.Buffer((4, 16384), \"float32\"), W: T.Buffer((32,), \"float32\"), B_unrolled: T.Buffer((16415,), \"float32\")):\n    T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"global_symbol\": \"default_function\", \"target\": T.target({\"arch\": \"sm_75\", \"host\": {\"keys\": [\"cpu\"], \"kind\": \"llvm\", \"tag\": \"\"}, \"keys\": [\"cuda\", \"gpu\"], \"kind\": \"cuda\", \"max_num_threads\": 1024, \"tag\": \"\", \"thread_warp_size\": 32}), \"tir.noalias\": T.bool(True)})\n    for n_inner in range(4):\n        B_unrolled_1 = T.Buffer((16415,), data=B_unrolled.data)\n        for n_outer_init in range(4104):\n            if n_outer_init * 4 + n_inner < 16415:\n                B_unrolled_1[n_outer_init * 4 + n_inner] = T.float32(0)\n        for k_inner, n_outer in T.grid(4, 4104):\n            if n_outer * 4 + n_inner < 16415:\n                cse_var_2: T.int32 = n_outer * 4\n                cse_var_1: T.int32 = cse_var_2 + n_inner\n                A_1 = T.Buffer((65536,), data=A.data)\n                W_1 = T.Buffer((32,), data=W.data)\n                B_unrolled_1[cse_var_1] = B_unrolled_1[cse_var_1] + A_1[k_inner * 16383 + cse_var_2 + n_inner] * W_1[k_inner]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "import numpy as np\n",
        "import sys\n",
        "# Adding assignment 3 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "from src.ops import make_conv1d_gpu_scheduler\n",
        "from tvm import te\n",
        "\n",
        "def make_conv1d_gpu_scheduler_optimized3(M, N):\n",
        "    A = te.placeholder((M,), name=\"A\")\n",
        "    W = te.placeholder((N,), name=\"W\")\n",
        "\n",
        "    k = te.reduce_axis((0, M + N - 1), \"k\")\n",
        "    B = te.compute(\n",
        "        (M + N - 1,),\n",
        "        lambda n: te.sum(tvm.tir.if_then_else(\n",
        "            tvm.tir.any(k < 0, k >= M, n - k < 0, n - k >= N),\n",
        "            tvm.tir.const(0.0, \"float32\"),\n",
        "            A[k] * W[n - k]), axis=k),\n",
        "        name=\"B\",\n",
        "    )\n",
        "\n",
        "    s = te.create_schedule(B.op)\n",
        "\n",
        "    # Optimization 1: Loop tiling\n",
        "    block_size = 32\n",
        "    bx, tx = s[B].split(B.op.axis[0], factor=block_size)\n",
        "    s[B].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
        "    s[B].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
        "\n",
        "\n",
        "\n",
        "    # Optimization 3: Vectorization\n",
        "    vector_length = 8\n",
        "    _, vec = s[B].split(tx, factor=vector_length)\n",
        "    s[B].vectorize(vec)\n",
        "\n",
        "    return s, A, W, B\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "M = 16384\n",
        "N = 32\n",
        "dtype = 'float32'\n",
        "a_np = np.random.rand(M).astype(dtype)\n",
        "w_np = np.random.rand(N).astype(dtype)\n",
        "b_np = np.convolve(a_np, w_np)\n",
        "\n",
        "s, A, W, B = make_conv1d_gpu_scheduler_optimized3(M, N) \n",
        "\n",
        "func = tvm.build(s, [A, W, B], \"cuda\")\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "a = tvm.nd.array(a_np, dev)\n",
        "w = tvm.nd.array(w_np, dev)\n",
        "b = tvm.nd.array(np.zeros((M+N-1), dtype), dev)\n",
        "func(a, w, b)\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1, repeat =1)\n",
        "\n",
        "\n",
        "print(\"Answer:\", b_np)\n",
        "print(\"Output:\", b)\n",
        "print(f\"1DConv TVM: %f ms\" % (evaluator(a, w, b).mean * 1e3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YFMSxt2Wtaz",
        "outputId": "4094ded4-c703-40e1-e395-c29ddbca2452"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: [0.04145448 0.73029566 0.85560024 ... 1.220298   0.93471885 0.41042155]\n",
            "Output: [0.04145448 0.73029566 0.8556003  ... 1.220298   0.93471885 0.41042155]\n",
            "1DConv TVM: 73.293052 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "import numpy as np\n",
        "import sys\n",
        "# Adding assignment 3 to the system path\n",
        "# Make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "from src.ops import make_conv1d_gpu_scheduler\n",
        "from tvm import te\n",
        "\n",
        "\n",
        "def make_conv1d_gpu_scheduler_optimized1(M, N):\n",
        "    A = te.placeholder((M,), name=\"A\")\n",
        "    W = te.placeholder((N,), name=\"W\")\n",
        "\n",
        "    k = te.reduce_axis((0, M + N - 1), \"k\")\n",
        "    B = te.compute(\n",
        "        (M + N - 1,),\n",
        "        lambda n: te.sum(tvm.tir.if_then_else(\n",
        "            tvm.tir.any(k < 0, k >= M, n - k < 0, n - k >= N),\n",
        "            tvm.tir.const(0.0, \"float32\"),\n",
        "            A[k] * W[n - k]), axis=k),\n",
        "        name=\"B\",\n",
        "    )\n",
        "\n",
        "    s = te.create_schedule(B.op)\n",
        "\n",
        "    # Optimization 1: Loop tiling\n",
        "    block_size = 32\n",
        "    bx, tx = s[B].split(B.op.axis[0], factor=block_size)\n",
        "    s[B].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
        "    s[B].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
        "    return s, A, W, B\n",
        "\n",
        "\n",
        "M = 16384\n",
        "N = 32\n",
        "dtype = 'float32'\n",
        "a_np = np.random.rand(M).astype(dtype)\n",
        "w_np = np.random.rand(N).astype(dtype)\n",
        "b_np = np.convolve(a_np, w_np)\n",
        "\n",
        "s, A, W, B = make_conv1d_gpu_scheduler_optimized1(M, N) \n",
        "\n",
        "func = tvm.build(s, [A, W, B], \"cuda\")\n",
        "\n",
        "dev = tvm.cuda(0)\n",
        "a = tvm.nd.array(a_np, dev)\n",
        "w = tvm.nd.array(w_np, dev)\n",
        "b = tvm.nd.array(np.zeros((M+N-1), dtype), dev)\n",
        "func(a, w, b)\n",
        "evaluator = func.time_evaluator(func.entry_name, dev, number=1, repeat =1)\n",
        "\n",
        "print(\"Answer:\", b_np)\n",
        "print(\"Output:\", b)\n",
        "print(f\"1DConv TVM: %f ms\" % (evaluator(a, w, b).mean * 1e3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKLjpZpMXU7q",
        "outputId": "2c707df6-4ec1-46ac-d1eb-8418285da7bc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: [0.16140658 0.09842803 0.8374316  ... 0.74492466 0.9198792  0.19857898]\n",
            "Output: [0.16140658 0.09842803 0.83743167 ... 0.74492466 0.9198792  0.19857898]\n",
            "1DConv TVM: 4.165728 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "dUT5WMw41ddU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95b9c6c-4a1f-4e17-802c-f5cf0cddc05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# from tvm.script import ir as I\n",
            "# from tvm.script import tir as T\n",
            "\n",
            "@I.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def main(A: T.Buffer((16384,), \"float32\"), W: T.Buffer((32,), \"float32\"), B: T.Buffer((16415,), \"float32\")):\n",
            "        T.func_attr({\"from_legacy_te_schedule\": T.bool(True), \"global_symbol\": \"main\", \"tir.noalias\": T.bool(True)})\n",
            "        blockIdx_x = T.launch_thread(\"blockIdx.x\", 513)\n",
            "        threadIdx_x = T.launch_thread(\"threadIdx.x\", 32)\n",
            "        B_1 = T.Buffer((16415,), data=B.data)\n",
            "        if T.likely(blockIdx_x * 32 + threadIdx_x < 16415):\n",
            "            B_1[blockIdx_x * 32 + threadIdx_x] = T.float32(0)\n",
            "        for k in range(16415):\n",
            "            if T.likely(blockIdx_x * 32 + threadIdx_x < 16415):\n",
            "                A_1 = T.Buffer((16384,), data=A.data)\n",
            "                W_1 = T.Buffer((32,), data=W.data)\n",
            "                B_1[blockIdx_x * 32 + threadIdx_x] = B_1[blockIdx_x * 32 + threadIdx_x] + T.if_then_else(16384 <= k or blockIdx_x * 32 + threadIdx_x - k < 0 or 1 <= (threadIdx_x - k) // 32 + blockIdx_x, T.float32(0), A_1[k] * W_1[blockIdx_x * 32 + threadIdx_x - k])\n"
          ]
        }
      ],
      "source": [
        "print(tvm.lower(s, [A, W, B], simple_mode=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "1_6kV5BU1ddU",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56bc44c-7d7c-4a28-88d4-650f299ce636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/ece5545/a3-swillenson\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.9.16, pytest-7.2.2, pluggy-1.0.0\n",
            "rootdir: /content/gdrive/MyDrive/ece5545/a3-swillenson\n",
            "plugins: anyio-3.6.2\n",
            "collected 15 items                                                             \u001b[0m\n",
            "\n",
            "tests/test_1dconv_gpu.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                 [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m============================= \u001b[32m\u001b[1m15 passed\u001b[0m\u001b[32m in 11.14s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "%cd {PROJECT_ROOT}\n",
        "!python -m pytest tests/test_1dconv_gpu.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_conv1d_gpu_scheduler(M, N):\n",
        "    A = te.placeholder((M,), name=\"A\")\n",
        "    W = te.placeholder((N,), name=\"W\")\n",
        "\n",
        "    k = te.reduce_axis((0, M + N - 1), \"k\")\n",
        "    B = te.compute(\n",
        "        (M + N - 1,),\n",
        "        lambda n: te.sum(tvm.tir.if_then_else(\n",
        "            tvm.tir.any(k < 0, k >= M, n - k < 0, n - k >= N),\n",
        "            tvm.tir.const(0.0, \"float32\"),\n",
        "            A[k] * W[n - k]), axis=k),\n",
        "        name=\"B\",\n",
        "    )\n",
        "\n",
        "    s = te.create_schedule(B.op)\n",
        "    \n",
        "\n",
        "    # Optimization 1: Loop tiling\n",
        "    block_size = 32\n",
        "    bx, tx = s[B].split(B.op.axis[0], factor=block_size)\n",
        "    s[B].bind(bx, te.thread_axis(\"blockIdx.x\"))\n",
        "    s[B].bind(tx, te.thread_axis(\"threadIdx.x\"))\n",
        "    return s, A, W, B\n"
      ],
      "metadata": {
        "id": "4XY6Oe5GXXdI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}